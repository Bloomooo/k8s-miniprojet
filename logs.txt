
==> Audit <==
|-----------|-----------------------------------------------------------|----------|-----------------------|---------|----------------------|----------------------|
|  Command  |                           Args                            | Profile  |         User          | Version |      Start Time      |       End Time       |
|-----------|-----------------------------------------------------------|----------|-----------------------|---------|----------------------|----------------------|
| start     | --driver=docker                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:25 CEST |                      |
| start     | --driver=hyperv                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:26 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:27 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:52 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:53 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:54 CEST | 07 Oct 24 10:55 CEST |
| kubectl   | -- get pods -A                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:56 CEST | 07 Oct 24 10:56 CEST |
| dashboard |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 10:57 CEST |                      |
| service   | hello-node                                                | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:00 CEST |                      |
| service   | hello-node                                                | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:00 CEST | 07 Oct 24 11:00 CEST |
| addons    | list                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:01 CEST | 07 Oct 24 11:01 CEST |
| addons    | enable metrics-server                                     | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:01 CEST | 07 Oct 24 11:01 CEST |
| addons    | disable metrics-server                                    | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:01 CEST | 07 Oct 24 11:02 CEST |
| dashboard |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 07 Oct 24 11:03 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:39 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:43 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:44 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:44 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:50 CEST |                      |
| stop      |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:58 CEST | 21 Oct 24 08:58 CEST |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 08:58 CEST | 21 Oct 24 09:00 CEST |
| service   | k8s-microproject-service --url                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:02 CEST |                      |
| service   | k8s-miniprojet-service --url                              | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:03 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:03 CEST |                      |
| service   | k8s-miniprojet-service --url                              | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:04 CEST |                      |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:05 CEST |                      |
| stop      |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:05 CEST | 21 Oct 24 09:05 CEST |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:05 CEST | 21 Oct 24 09:06 CEST |
| service   | k8s-microproject-service --url                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:07 CEST |                      |
| service   | k8s-miniprojet-service --url                              | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:07 CEST | 21 Oct 24 09:07 CEST |
| ip        |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:15 CEST | 21 Oct 24 09:15 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:17 CEST |                      |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:19 CEST |                      |
| addons    | list                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:25 CEST | 21 Oct 24 09:25 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:26 CEST |                      |
| addons    | disable ingress                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:26 CEST | 21 Oct 24 09:26 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:26 CEST |                      |
| stop      |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:38 CEST | 21 Oct 24 09:39 CEST |
| start     | --memory=4096 --cpus=2                                    | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:39 CEST | 21 Oct 24 09:40 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:40 CEST |                      |
| addons    | enable metrics-server                                     | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:41 CEST | 21 Oct 24 09:41 CEST |
| addons    | list                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:41 CEST | 21 Oct 24 09:41 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:41 CEST |                      |
| addons    | disable ingress                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:46 CEST | 21 Oct 24 09:46 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:46 CEST |                      |
| stop      |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:47 CEST | 21 Oct 24 09:48 CEST |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:48 CEST | 21 Oct 24 09:49 CEST |
| addons    | disable ingress                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:49 CEST | 21 Oct 24 09:49 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:49 CEST |                      |
| addons    | list                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:52 CEST | 21 Oct 24 09:52 CEST |
| ssh       |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:56 CEST | 21 Oct 24 09:57 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 09:57 CEST |                      |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:03 CEST |                      |
| image     | load                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:07 CEST | 21 Oct 24 10:07 CEST |
|           | registry.k8s.io/ingress-nginx/controller:v1.11.2          |          |                       |         |                      |                      |
| image     | load                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:07 CEST | 21 Oct 24 10:08 CEST |
|           | registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3 |          |                       |         |                      |                      |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:08 CEST |                      |
| stop      |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:09 CEST | 21 Oct 24 10:09 CEST |
| start     |                                                           | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:10 CEST | 21 Oct 24 10:10 CEST |
| image     | list                                                      | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:12 CEST | 21 Oct 24 10:12 CEST |
| addons    | enable ingress                                            | minikube | DESKTOP-PE2TGQR\mecht | v1.34.0 | 21 Oct 24 10:12 CEST |                      |
|-----------|-----------------------------------------------------------|----------|-----------------------|---------|----------------------|----------------------|


==> Dernier démarrage <==
Log file created at: 2024/10/21 10:10:01
Running on machine: DESKTOP-PE2TGQR
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1021 10:10:01.770721   17860 out.go:345] Setting OutFile to fd 100 ...
I1021 10:10:01.788769   17860 out.go:392] TERM=,COLORTERM=, which probably does not support color
I1021 10:10:01.788769   17860 out.go:358] Setting ErrFile to fd 104...
I1021 10:10:01.788769   17860 out.go:392] TERM=,COLORTERM=, which probably does not support color
W1021 10:10:01.804230   17860 root.go:314] Error reading config file at C:\Users\mecht\.minikube\config\config.json: open C:\Users\mecht\.minikube\config\config.json: The system cannot find the file specified.
I1021 10:10:01.808423   17860 out.go:352] Setting JSON to false
I1021 10:10:01.814785   17860 start.go:129] hostinfo: {"hostname":"DESKTOP-PE2TGQR","uptime":154160,"bootTime":1729344041,"procs":377,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4249 Build 22631.4249","kernelVersion":"10.0.22631.4249 Build 22631.4249","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"8c56bde1-eb05-4edf-b859-29034e32a895"}
W1021 10:10:01.814785   17860 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1021 10:10:01.826649   17860 out.go:177] * minikube v1.34.0 sur Microsoft Windows 11 Home 10.0.22631.4249 Build 22631.4249
I1021 10:10:01.827728   17860 notify.go:220] Checking for updates...
I1021 10:10:01.827838   17860 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1021 10:10:01.828342   17860 driver.go:394] Setting default libvirt URI to qemu:///system
I1021 10:10:02.855576   17860 out.go:177] * Utilisation du pilote hyperv basé sur le profil existant
I1021 10:10:02.860254   17860 start.go:297] selected driver: hyperv
I1021 10:10:02.860254   17860 start.go:901] validating driver "hyperv" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.20.118.161 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mecht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1021 10:10:02.860254   17860 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1021 10:10:02.909902   17860 cni.go:84] Creating CNI manager for ""
I1021 10:10:02.909902   17860 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1021 10:10:02.909902   17860 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.20.118.161 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mecht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1021 10:10:02.910423   17860 iso.go:125] acquiring lock: {Name:mk3f76bef0b43cd233eeab3b8638118ff2fda1e2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1021 10:10:02.911452   17860 out.go:177] * Démarrage du nœud "minikube" primary control-plane dans le cluster "minikube"
I1021 10:10:02.911971   17860 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1021 10:10:02.912567   17860 preload.go:146] Found local preload: C:\Users\mecht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1021 10:10:02.912567   17860 cache.go:56] Caching tarball of preloaded images
I1021 10:10:02.912567   17860 preload.go:172] Found C:\Users\mecht\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1021 10:10:02.912567   17860 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1021 10:10:02.913085   17860 profile.go:143] Saving config to C:\Users\mecht\.minikube\profiles\minikube\config.json ...
I1021 10:10:02.914651   17860 start.go:360] acquireMachinesLock for minikube: {Name:mk2a903906b084b8d54964c7032c423d50b7695b Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1021 10:10:02.914651   17860 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1021 10:10:02.914651   17860 start.go:96] Skipping create...Using existing machine configuration
I1021 10:10:02.914651   17860 fix.go:54] fixHost starting: 
I1021 10:10:02.915165   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:03.383609   17860 main.go:141] libmachine: [stdout =====>] : Off

I1021 10:10:03.383609   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:03.383609   17860 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1021 10:10:03.383609   17860 fix.go:138] unexpected machine state, will restart: <nil>
I1021 10:10:03.385214   17860 out.go:177] * Redémarrage du hyperv VM existant pour "minikube" ...
I1021 10:10:03.386261   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Start-VM minikube
I1021 10:10:04.590118   17860 main.go:141] libmachine: [stdout =====>] : 
I1021 10:10:04.590118   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:04.590118   17860 main.go:141] libmachine: Waiting for host to start...
I1021 10:10:04.590118   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:05.086637   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:05.086637   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:05.086637   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:05.799229   17860 main.go:141] libmachine: [stdout =====>] : 
I1021 10:10:05.799229   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:06.965350   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:07.591270   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:07.591270   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:07.591270   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:08.223440   17860 main.go:141] libmachine: [stdout =====>] : 
I1021 10:10:08.223440   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:09.230576   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:09.720871   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:09.720871   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:09.720871   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:10.624320   17860 main.go:141] libmachine: [stdout =====>] : 
I1021 10:10:10.624320   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:11.624889   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:12.103676   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:12.103676   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:12.103676   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:12.782451   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:12.782451   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:12.784508   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:13.297433   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:13.298444   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:13.298444   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:13.963169   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:13.963169   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:13.963373   17860 profile.go:143] Saving config to C:\Users\mecht\.minikube\profiles\minikube\config.json ...
I1021 10:10:13.972107   17860 machine.go:93] provisionDockerMachine start ...
I1021 10:10:13.972107   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:14.549777   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:14.549777   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:14.549777   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:15.247320   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:15.247320   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:15.250797   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:15.250973   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:15.250973   17860 main.go:141] libmachine: About to run SSH command:
hostname
I1021 10:10:15.347677   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1021 10:10:15.347677   17860 buildroot.go:166] provisioning hostname "minikube"
I1021 10:10:15.347677   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:15.839931   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:15.839931   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:15.839931   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:16.465494   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:16.465494   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:16.469918   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:16.470484   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:16.470484   17860 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1021 10:10:16.571525   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1021 10:10:16.571525   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:17.092789   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:17.092789   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:17.092789   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:17.735491   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:17.735491   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:17.738767   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:17.739416   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:17.739416   17860 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1021 10:10:17.837474   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1021 10:10:17.837474   17860 buildroot.go:172] set auth options {CertDir:C:\Users\mecht\.minikube CaCertPath:C:\Users\mecht\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\mecht\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\mecht\.minikube\machines\server.pem ServerKeyPath:C:\Users\mecht\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\mecht\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\mecht\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\mecht\.minikube}
I1021 10:10:17.837474   17860 buildroot.go:174] setting up certificates
I1021 10:10:17.837474   17860 provision.go:84] configureAuth start
I1021 10:10:17.837474   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:18.302773   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:18.302773   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:18.302773   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:18.937558   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:18.937558   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:18.937558   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:19.405560   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:19.405560   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:19.405560   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:20.051631   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:20.051631   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:20.051631   17860 provision.go:143] copyHostCerts
I1021 10:10:20.062303   17860 exec_runner.go:144] found C:\Users\mecht\.minikube/ca.pem, removing ...
I1021 10:10:20.062303   17860 exec_runner.go:203] rm: C:\Users\mecht\.minikube\ca.pem
I1021 10:10:20.062303   17860 exec_runner.go:151] cp: C:\Users\mecht\.minikube\certs\ca.pem --> C:\Users\mecht\.minikube/ca.pem (1074 bytes)
I1021 10:10:20.064114   17860 exec_runner.go:144] found C:\Users\mecht\.minikube/cert.pem, removing ...
I1021 10:10:20.064114   17860 exec_runner.go:203] rm: C:\Users\mecht\.minikube\cert.pem
I1021 10:10:20.064114   17860 exec_runner.go:151] cp: C:\Users\mecht\.minikube\certs\cert.pem --> C:\Users\mecht\.minikube/cert.pem (1119 bytes)
I1021 10:10:20.064715   17860 exec_runner.go:144] found C:\Users\mecht\.minikube/key.pem, removing ...
I1021 10:10:20.064715   17860 exec_runner.go:203] rm: C:\Users\mecht\.minikube\key.pem
I1021 10:10:20.065233   17860 exec_runner.go:151] cp: C:\Users\mecht\.minikube\certs\key.pem --> C:\Users\mecht\.minikube/key.pem (1675 bytes)
I1021 10:10:20.065766   17860 provision.go:117] generating server cert: C:\Users\mecht\.minikube\machines\server.pem ca-key=C:\Users\mecht\.minikube\certs\ca.pem private-key=C:\Users\mecht\.minikube\certs\ca-key.pem org=mecht.minikube san=[127.0.0.1 172.20.119.178 localhost minikube]
I1021 10:10:20.184930   17860 provision.go:177] copyRemoteCerts
I1021 10:10:20.204729   17860 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1021 10:10:20.204729   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:20.687922   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:20.687922   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:20.687922   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:21.378530   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:21.378530   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:21.378530   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:21.457618   17860 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.253066s)
I1021 10:10:21.457618   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1021 10:10:21.478262   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1021 10:10:21.495918   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1021 10:10:21.516499   17860 provision.go:87] duration metric: took 3.6795439s to configureAuth
I1021 10:10:21.516499   17860 buildroot.go:189] setting minikube options for container-runtime
I1021 10:10:21.516499   17860 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1021 10:10:21.516499   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:22.003794   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:22.003794   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:22.003794   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:22.739953   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:22.739953   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:22.743231   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:22.743744   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:22.743744   17860 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1021 10:10:22.845227   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1021 10:10:22.845227   17860 buildroot.go:70] root file system type: tmpfs
I1021 10:10:22.845227   17860 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1021 10:10:22.845227   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:23.351841   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:23.351841   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:23.351841   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:24.009998   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:24.009998   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:24.013836   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:24.013836   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:24.013836   17860 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1021 10:10:24.116148   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1021 10:10:24.116148   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:24.603650   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:24.603650   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:24.603650   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:25.357839   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:25.357839   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:25.362110   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:25.362652   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:25.362652   17860 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1021 10:10:27.129656   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I1021 10:10:27.129656   17860 machine.go:96] duration metric: took 13.1594029s to provisionDockerMachine
I1021 10:10:27.130660   17860 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I1021 10:10:27.130660   17860 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1021 10:10:27.148726   17860 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1021 10:10:27.148726   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:27.640910   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:27.640910   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:27.640910   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:28.282921   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:28.282921   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:28.282921   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:28.367095   17860 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.2185401s)
I1021 10:10:28.375933   17860 ssh_runner.go:195] Run: cat /etc/os-release
I1021 10:10:28.378798   17860 info.go:137] Remote host: Buildroot 2023.02.9
I1021 10:10:28.378798   17860 filesync.go:126] Scanning C:\Users\mecht\.minikube\addons for local assets ...
I1021 10:10:28.378798   17860 filesync.go:126] Scanning C:\Users\mecht\.minikube\files for local assets ...
I1021 10:10:28.378798   17860 start.go:296] duration metric: took 1.2483143s for postStartSetup
I1021 10:10:28.379313   17860 fix.go:56] duration metric: took 25.4682507s for fixHost
I1021 10:10:28.379313   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:28.843903   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:28.843903   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:28.843903   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:29.468712   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:29.468712   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:29.472638   17860 main.go:141] libmachine: Using SSH client type: native
I1021 10:10:29.473714   17860 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x141c9c0] 0x141f5a0 <nil>  [] 0s} 172.20.119.178 22 <nil> <nil>}
I1021 10:10:29.473714   17860 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I1021 10:10:29.563456   17860 main.go:141] libmachine: SSH cmd err, output: <nil>: 1729498229.066930161

I1021 10:10:29.563456   17860 fix.go:216] guest clock: 1729498229.066930161
I1021 10:10:29.563456   17860 fix.go:229] Guest: 2024-10-21 10:10:29.066930161 +0200 CEST Remote: 2024-10-21 10:10:28.3793138 +0200 CEST m=+26.673165901 (delta=687.616361ms)
I1021 10:10:29.563456   17860 fix.go:200] guest clock delta is within tolerance: 687.616361ms
I1021 10:10:29.563456   17860 start.go:83] releasing machines lock for "minikube", held for 26.65256s
I1021 10:10:29.563456   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:30.042809   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:30.042809   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:30.042809   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:30.670939   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:30.670939   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:30.674629   17860 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1021 10:10:30.674629   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:30.681649   17860 ssh_runner.go:195] Run: cat /version.json
I1021 10:10:30.681649   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:31.178094   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:31.178094   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:31.178094   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:31.189336   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:31.189336   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:31.189336   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:31.900520   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:31.900520   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:31.900520   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:31.912571   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:31.912571   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:31.912571   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:31.975453   17860 ssh_runner.go:235] Completed: cat /version.json: (1.2939867s)
I1021 10:10:31.988838   17860 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (1.3143942s)
W1021 10:10:31.988838   17860 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1021 10:10:31.994158   17860 ssh_runner.go:195] Run: systemctl --version
I1021 10:10:32.007640   17860 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1021 10:10:32.010042   17860 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1021 10:10:32.029955   17860 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1021 10:10:32.040053   17860 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1021 10:10:32.040053   17860 start.go:495] detecting cgroup driver to use...
I1021 10:10:32.040053   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1021 10:10:32.059397   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1021 10:10:32.075680   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1021 10:10:32.082441   17860 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1021 10:10:32.091423   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1021 10:10:32.106568   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1021 10:10:32.122067   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1021 10:10:32.137772   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1021 10:10:32.154579   17860 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1021 10:10:32.169958   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1021 10:10:32.186966   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1021 10:10:32.202224   17860 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1021 10:10:32.227311   17860 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1021 10:10:32.253252   17860 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1021 10:10:32.279512   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:32.378511   17860 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1021 10:10:32.392357   17860 start.go:495] detecting cgroup driver to use...
I1021 10:10:32.410633   17860 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1021 10:10:32.439395   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1021 10:10:32.466557   17860 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1021 10:10:32.497835   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1021 10:10:32.526244   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1021 10:10:32.553492   17860 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1021 10:10:32.603347   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1021 10:10:32.613179   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1021 10:10:32.634635   17860 ssh_runner.go:195] Run: which cri-dockerd
I1021 10:10:32.655108   17860 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1021 10:10:32.660523   17860 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1021 10:10:32.690596   17860 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1021 10:10:32.786389   17860 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1021 10:10:32.859820   17860 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1021 10:10:32.859820   17860 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1021 10:10:32.890573   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:32.978820   17860 ssh_runner.go:195] Run: sudo systemctl restart docker
W1021 10:10:33.340058   17860 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W1021 10:10:33.341563   17860 out.go:270] * Pour extraire de nouvelles images externes, vous devrez peut-être configurer un proxy : https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1021 10:10:35.322177   17860 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.3436865s)
I1021 10:10:35.350512   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1021 10:10:35.392789   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1021 10:10:35.430585   17860 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1021 10:10:35.553376   17860 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1021 10:10:35.654444   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:35.774516   17860 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1021 10:10:35.813603   17860 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1021 10:10:35.852294   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:35.969277   17860 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1021 10:10:36.013670   17860 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1021 10:10:36.026372   17860 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1021 10:10:36.030264   17860 start.go:563] Will wait 60s for crictl version
I1021 10:10:36.039344   17860 ssh_runner.go:195] Run: which crictl
I1021 10:10:36.061335   17860 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1021 10:10:36.089314   17860 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1021 10:10:36.109012   17860 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1021 10:10:36.139962   17860 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1021 10:10:36.157862   17860 out.go:235] * Préparation de Kubernetes v1.31.0 sur Docker 27.2.0...
I1021 10:10:36.158392   17860 ip.go:176] getIPForInterface: searching for "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Connexion au réseau local* 1" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Connexion au réseau local* 2" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Wi-Fi" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "VMware Network Adapter VMnet1" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "VMware Network Adapter VMnet8" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Connexion réseau Bluetooth" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Ethernet" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:190] "Loopback Pseudo-Interface 1" does not match prefix "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:185] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I1021 10:10:36.165469   17860 ip.go:211] Found interface: {Index:43 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:b7:4d:67 Flags:up|broadcast|multicast|running}
I1021 10:10:36.171936   17860 ip.go:214] interface addr: fe80::c94b:269d:8f4e:6e5/64
I1021 10:10:36.171936   17860 ip.go:214] interface addr: 172.20.112.1/20
I1021 10:10:36.180720   17860 ssh_runner.go:195] Run: grep 172.20.112.1	host.minikube.internal$ /etc/hosts
I1021 10:10:36.183800   17860 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "172.20.112.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1021 10:10:36.191312   17860 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.20.119.178 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mecht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1021 10:10:36.191816   17860 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1021 10:10:36.205489   17860 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1021 10:10:36.217434   17860 docker.go:685] Got preloaded images: -- stdout --
blooomoo/k8s-microproject:latest
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/ingress-nginx/controller:v1.11.2
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1021 10:10:36.217434   17860 docker.go:615] Images already preloaded, skipping extraction
I1021 10:10:36.231112   17860 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1021 10:10:36.243363   17860 docker.go:685] Got preloaded images: -- stdout --
blooomoo/k8s-microproject:latest
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/ingress-nginx/controller:v1.11.2
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.3
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1021 10:10:36.243363   17860 cache_images.go:84] Images are preloaded, skipping loading
I1021 10:10:36.243363   17860 kubeadm.go:934] updating node { 172.20.119.178 8443 v1.31.0 docker true true} ...
I1021 10:10:36.243363   17860 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.20.119.178

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1021 10:10:36.265031   17860 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1021 10:10:36.300824   17860 cni.go:84] Creating CNI manager for ""
I1021 10:10:36.300824   17860 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1021 10:10:36.300824   17860 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1021 10:10:36.300824   17860 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.20.119.178 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.20.119.178"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.20.119.178 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1021 10:10:36.300824   17860 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.20.119.178
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.20.119.178
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.20.119.178"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1021 10:10:36.319949   17860 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1021 10:10:36.330227   17860 binaries.go:44] Found k8s binaries, skipping transfer
I1021 10:10:36.348238   17860 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1021 10:10:36.355473   17860 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (309 bytes)
I1021 10:10:36.366491   17860 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1021 10:10:36.377156   17860 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2156 bytes)
I1021 10:10:36.405448   17860 ssh_runner.go:195] Run: grep 172.20.119.178	control-plane.minikube.internal$ /etc/hosts
I1021 10:10:36.407592   17860 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "172.20.119.178	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1021 10:10:36.433772   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:36.525793   17860 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1021 10:10:36.537515   17860 certs.go:68] Setting up C:\Users\mecht\.minikube\profiles\minikube for IP: 172.20.119.178
I1021 10:10:36.537515   17860 certs.go:194] generating shared ca certs ...
I1021 10:10:36.537515   17860 certs.go:226] acquiring lock for ca certs: {Name:mk26d21f70f42998f3844c748686b5d3da4172dc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:36.540208   17860 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\mecht\.minikube\ca.key
I1021 10:10:36.541796   17860 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\mecht\.minikube\proxy-client-ca.key
I1021 10:10:36.541796   17860 certs.go:256] generating profile certs ...
I1021 10:10:36.542339   17860 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\mecht\.minikube\profiles\minikube\client.key
I1021 10:10:36.542339   17860 certs.go:363] generating signed profile cert for "minikube": C:\Users\mecht\.minikube\profiles\minikube\apiserver.key.27d24c02
I1021 10:10:36.542339   17860 crypto.go:68] Generating cert C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt.27d24c02 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 172.20.119.178]
I1021 10:10:36.734717   17860 crypto.go:156] Writing cert to C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt.27d24c02 ...
I1021 10:10:36.734717   17860 lock.go:35] WriteFile acquiring C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt.27d24c02: {Name:mkbb8bc81e05e8646447cce019d5d8292cda5e40 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:36.734717   17860 crypto.go:164] Writing key to C:\Users\mecht\.minikube\profiles\minikube\apiserver.key.27d24c02 ...
I1021 10:10:36.734717   17860 lock.go:35] WriteFile acquiring C:\Users\mecht\.minikube\profiles\minikube\apiserver.key.27d24c02: {Name:mkb58b0800783cfb7980d6eb6e6502ecf16c29e8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:36.735721   17860 certs.go:381] copying C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt.27d24c02 -> C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt
I1021 10:10:36.742226   17860 certs.go:385] copying C:\Users\mecht\.minikube\profiles\minikube\apiserver.key.27d24c02 -> C:\Users\mecht\.minikube\profiles\minikube\apiserver.key
I1021 10:10:36.743233   17860 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\mecht\.minikube\profiles\minikube\proxy-client.key
I1021 10:10:36.744228   17860 certs.go:484] found cert: C:\Users\mecht\.minikube\certs\ca-key.pem (1675 bytes)
I1021 10:10:36.744228   17860 certs.go:484] found cert: C:\Users\mecht\.minikube\certs\ca.pem (1074 bytes)
I1021 10:10:36.744228   17860 certs.go:484] found cert: C:\Users\mecht\.minikube\certs\cert.pem (1119 bytes)
I1021 10:10:36.744228   17860 certs.go:484] found cert: C:\Users\mecht\.minikube\certs\key.pem (1675 bytes)
I1021 10:10:36.745559   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1021 10:10:36.761589   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1021 10:10:36.778303   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1021 10:10:36.793196   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1021 10:10:36.808200   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1021 10:10:36.824005   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1021 10:10:36.838932   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1021 10:10:36.853506   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1021 10:10:36.867959   17860 ssh_runner.go:362] scp C:\Users\mecht\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1021 10:10:36.883097   17860 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I1021 10:10:36.901612   17860 ssh_runner.go:195] Run: openssl version
I1021 10:10:36.924535   17860 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1021 10:10:36.940925   17860 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1021 10:10:36.944279   17860 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct  7 08:55 /usr/share/ca-certificates/minikubeCA.pem
I1021 10:10:36.953079   17860 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1021 10:10:36.975255   17860 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1021 10:10:36.992839   17860 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1021 10:10:37.005211   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1021 10:10:37.018579   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1021 10:10:37.032589   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1021 10:10:37.046223   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1021 10:10:37.059461   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1021 10:10:37.073228   17860 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1021 10:10:37.077368   17860 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.20.119.178 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mecht:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1021 10:10:37.092365   17860 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1021 10:10:37.122438   17860 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1021 10:10:37.129425   17860 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1021 10:10:37.129930   17860 kubeadm.go:593] restartPrimaryControlPlane start ...
I1021 10:10:37.148638   17860 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1021 10:10:37.155962   17860 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1021 10:10:37.156470   17860 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\mecht\.kube\config
I1021 10:10:37.156470   17860 kubeconfig.go:62] C:\Users\mecht\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1021 10:10:37.156470   17860 lock.go:35] WriteFile acquiring C:\Users\mecht\.kube\config: {Name:mkaca3244c8a8506b6042a979d7077a63d29a592 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:37.183096   17860 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1021 10:10:37.189759   17860 kubeadm.go:640] detected kubeadm config drift (will reconfigure cluster from new /var/tmp/minikube/kubeadm.yaml):
-- stdout --
--- /var/tmp/minikube/kubeadm.yaml
+++ /var/tmp/minikube/kubeadm.yaml.new
@@ -1,7 +1,7 @@
 apiVersion: kubeadm.k8s.io/v1beta3
 kind: InitConfiguration
 localAPIEndpoint:
-  advertiseAddress: 172.20.118.161
+  advertiseAddress: 172.20.119.178
   bindPort: 8443
 bootstrapTokens:
   - groups:
@@ -14,13 +14,13 @@
   criSocket: unix:///var/run/cri-dockerd.sock
   name: "minikube"
   kubeletExtraArgs:
-    node-ip: 172.20.118.161
+    node-ip: 172.20.119.178
   taints: []
 ---
 apiVersion: kubeadm.k8s.io/v1beta3
 kind: ClusterConfiguration
 apiServer:
-  certSANs: ["127.0.0.1", "localhost", "172.20.118.161"]
+  certSANs: ["127.0.0.1", "localhost", "172.20.119.178"]
   extraArgs:
     enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
 controllerManager:

-- /stdout --
I1021 10:10:37.189759   17860 kubeadm.go:1160] stopping kube-system containers ...
I1021 10:10:37.204975   17860 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1021 10:10:37.218158   17860 docker.go:483] Stopping containers: [9cfa5fc12a06 38c04065d2b2 3b47b27eca9a db876e4fa97e 0fa830d21ea9 5f92148b60de d5a5705a5966 9ff7ea0a234e 85eade8e9a80 2bbd46733dfa cfe846b8804b 73abcdc61dfb e1eb455dd9de 20f7a6faef2d c091ebae4d90 5f510e7d2c8d dde92f1b3a2c 3b2cc541663e b5703c595c67 db57c85929df a5d5b6fe40af 5fef34e2845c 350554a4f50b 749ecea1ce3d 2b40cf94dbc8 ffb121dff631 4f9458db74e4 1684a48f3c74 d498d1deda7c 370d8520f85c 73902998c3ef]
I1021 10:10:37.233074   17860 ssh_runner.go:195] Run: docker stop 9cfa5fc12a06 38c04065d2b2 3b47b27eca9a db876e4fa97e 0fa830d21ea9 5f92148b60de d5a5705a5966 9ff7ea0a234e 85eade8e9a80 2bbd46733dfa cfe846b8804b 73abcdc61dfb e1eb455dd9de 20f7a6faef2d c091ebae4d90 5f510e7d2c8d dde92f1b3a2c 3b2cc541663e b5703c595c67 db57c85929df a5d5b6fe40af 5fef34e2845c 350554a4f50b 749ecea1ce3d 2b40cf94dbc8 ffb121dff631 4f9458db74e4 1684a48f3c74 d498d1deda7c 370d8520f85c 73902998c3ef
I1021 10:10:37.266333   17860 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1021 10:10:37.298653   17860 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1021 10:10:37.306664   17860 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1021 10:10:37.306664   17860 kubeadm.go:157] found existing configuration files:

I1021 10:10:37.325590   17860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1021 10:10:37.332614   17860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1021 10:10:37.350995   17860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1021 10:10:37.378553   17860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1021 10:10:37.386247   17860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1021 10:10:37.409677   17860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1021 10:10:37.438352   17860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1021 10:10:37.444671   17860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1021 10:10:37.465214   17860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1021 10:10:37.491741   17860 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1021 10:10:37.500997   17860 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1021 10:10:37.522252   17860 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1021 10:10:37.549865   17860 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1021 10:10:37.557955   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:37.722427   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:38.393367   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:38.519235   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:38.569713   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:38.606842   17860 api_server.go:52] waiting for apiserver process to appear ...
I1021 10:10:38.626171   17860 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1021 10:10:39.132244   17860 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1021 10:10:39.630120   17860 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1021 10:10:39.639004   17860 api_server.go:72] duration metric: took 1.0323076s to wait for apiserver process to appear ...
I1021 10:10:39.639004   17860 api_server.go:88] waiting for apiserver healthz status ...
I1021 10:10:39.639004   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:41.830851   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1021 10:10:41.830851   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1021 10:10:41.830851   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:41.894378   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:41.894378   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:42.138994   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:42.142800   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:42.142800   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:42.651387   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:42.655060   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:42.655060   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:43.148148   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:43.151990   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:43.151990   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:43.646266   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:43.649786   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:43.649786   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:44.144599   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:44.148251   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:44.148251   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:44.639409   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:44.643194   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:44.643194   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:45.148269   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:45.152680   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:45.152680   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:45.641978   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:45.645899   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:45.645899   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:46.152225   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:46.155785   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:46.155785   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:46.647339   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:46.651249   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1021 10:10:46.651249   17860 api_server.go:103] status: https://172.20.119.178:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1021 10:10:47.142604   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:47.145863   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 200:
ok
I1021 10:10:47.151671   17860 api_server.go:141] control plane version: v1.31.0
I1021 10:10:47.151671   17860 api_server.go:131] duration metric: took 7.513726s to wait for apiserver health ...
I1021 10:10:47.151671   17860 cni.go:84] Creating CNI manager for ""
I1021 10:10:47.151671   17860 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1021 10:10:47.162015   17860 out.go:177] * Configuration de bridge CNI (Container Networking Interface)...
I1021 10:10:47.182968   17860 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1021 10:10:47.188664   17860 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1021 10:10:47.198447   17860 system_pods.go:43] waiting for kube-system pods to appear ...
I1021 10:10:47.204908   17860 system_pods.go:59] 9 kube-system pods found
I1021 10:10:47.204908   17860 system_pods.go:61] "coredns-6f6b679f8f-2fvhb" [bd9148c2-9c91-4e5d-aeec-35c272a80029] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1021 10:10:47.204908   17860 system_pods.go:61] "coredns-6f6b679f8f-x9v65" [0c3f202c-c074-464c-b69b-50411667ba4f] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1021 10:10:47.204908   17860 system_pods.go:61] "etcd-minikube" [e46f8108-7641-4ba0-8062-ab53213e0a7a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1021 10:10:47.205456   17860 system_pods.go:61] "kube-apiserver-minikube" [94a92a99-aa8a-4988-8b58-98c6273459d8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1021 10:10:47.205456   17860 system_pods.go:61] "kube-controller-manager-minikube" [68f00ab4-6210-4da2-b97b-3f14a89e98ad] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1021 10:10:47.205456   17860 system_pods.go:61] "kube-proxy-kgqrd" [82823504-b40a-439b-89d0-197afa4aade0] Running
I1021 10:10:47.205456   17860 system_pods.go:61] "kube-scheduler-minikube" [8bda4a3f-c95a-4c64-8672-92364fbc7c3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1021 10:10:47.205456   17860 system_pods.go:61] "metrics-server-84c5f94fbc-dxqmp" [2dc37bcd-0951-4b9d-a775-44b2c67c2b98] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1021 10:10:47.205456   17860 system_pods.go:61] "storage-provisioner" [edbeda38-f7e9-4579-b609-c7d287c69e63] Running
I1021 10:10:47.205456   17860 system_pods.go:74] duration metric: took 7.0102ms to wait for pod list to return data ...
I1021 10:10:47.205456   17860 node_conditions.go:102] verifying NodePressure condition ...
I1021 10:10:47.207507   17860 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I1021 10:10:47.207507   17860 node_conditions.go:123] node cpu capacity is 2
I1021 10:10:47.207507   17860 node_conditions.go:105] duration metric: took 2.0516ms to run NodePressure ...
I1021 10:10:47.207507   17860 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1021 10:10:47.485240   17860 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1021 10:10:47.491661   17860 ops.go:34] apiserver oom_adj: -16
I1021 10:10:47.491661   17860 kubeadm.go:597] duration metric: took 10.3631905s to restartPrimaryControlPlane
I1021 10:10:47.491661   17860 kubeadm.go:394] duration metric: took 10.4157602s to StartCluster
I1021 10:10:47.491661   17860 settings.go:142] acquiring lock: {Name:mk0c762a94e7d4cc3a24b8ab3a0ebfe2146ed2ee Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:47.491661   17860 settings.go:150] Updating kubeconfig:  C:\Users\mecht\.kube\config
I1021 10:10:47.492165   17860 lock.go:35] WriteFile acquiring C:\Users\mecht\.kube\config: {Name:mkaca3244c8a8506b6042a979d7077a63d29a592 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1021 10:10:47.492683   17860 start.go:235] Will wait 6m0s for node &{Name: IP:172.20.119.178 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1021 10:10:47.492683   17860 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1021 10:10:47.492683   17860 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1021 10:10:47.492683   17860 addons.go:69] Setting dashboard=true in profile "minikube"
I1021 10:10:47.492683   17860 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1021 10:10:47.492683   17860 addons.go:234] Setting addon dashboard=true in "minikube"
W1021 10:10:47.492683   17860 addons.go:243] addon storage-provisioner should already be in state true
W1021 10:10:47.492683   17860 addons.go:243] addon dashboard should already be in state true
I1021 10:10:47.492683   17860 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1021 10:10:47.492683   17860 addons.go:69] Setting metrics-server=true in profile "minikube"
I1021 10:10:47.492683   17860 addons.go:234] Setting addon metrics-server=true in "minikube"
W1021 10:10:47.492683   17860 addons.go:243] addon metrics-server should already be in state true
I1021 10:10:47.492683   17860 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1021 10:10:47.493197   17860 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1021 10:10:47.493197   17860 out.go:177] * Vérification des composants Kubernetes...
I1021 10:10:47.493197   17860 host.go:66] Checking if "minikube" exists ...
I1021 10:10:47.493197   17860 host.go:66] Checking if "minikube" exists ...
I1021 10:10:47.493197   17860 host.go:66] Checking if "minikube" exists ...
I1021 10:10:47.493722   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:47.494244   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:47.494244   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:47.494244   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:47.531733   17860 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1021 10:10:47.752664   17860 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1021 10:10:47.764829   17860 api_server.go:52] waiting for apiserver process to appear ...
I1021 10:10:47.791705   17860 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1021 10:10:47.802817   17860 api_server.go:72] duration metric: took 310.1774ms to wait for apiserver process to appear ...
I1021 10:10:47.802817   17860 api_server.go:88] waiting for apiserver healthz status ...
I1021 10:10:47.802817   17860 api_server.go:253] Checking apiserver healthz at https://172.20.119.178:8443/healthz ...
I1021 10:10:47.806301   17860 api_server.go:279] https://172.20.119.178:8443/healthz returned 200:
ok
I1021 10:10:47.807654   17860 api_server.go:141] control plane version: v1.31.0
I1021 10:10:47.807654   17860 api_server.go:131] duration metric: took 4.8376ms to wait for apiserver health ...
I1021 10:10:47.807654   17860 system_pods.go:43] waiting for kube-system pods to appear ...
I1021 10:10:47.812412   17860 system_pods.go:59] 9 kube-system pods found
I1021 10:10:47.812412   17860 system_pods.go:61] "coredns-6f6b679f8f-2fvhb" [bd9148c2-9c91-4e5d-aeec-35c272a80029] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1021 10:10:47.812412   17860 system_pods.go:61] "coredns-6f6b679f8f-x9v65" [0c3f202c-c074-464c-b69b-50411667ba4f] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1021 10:10:47.812927   17860 system_pods.go:61] "etcd-minikube" [e46f8108-7641-4ba0-8062-ab53213e0a7a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1021 10:10:47.812927   17860 system_pods.go:61] "kube-apiserver-minikube" [94a92a99-aa8a-4988-8b58-98c6273459d8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1021 10:10:47.812927   17860 system_pods.go:61] "kube-controller-manager-minikube" [68f00ab4-6210-4da2-b97b-3f14a89e98ad] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1021 10:10:47.812927   17860 system_pods.go:61] "kube-proxy-kgqrd" [82823504-b40a-439b-89d0-197afa4aade0] Running
I1021 10:10:47.812927   17860 system_pods.go:61] "kube-scheduler-minikube" [8bda4a3f-c95a-4c64-8672-92364fbc7c3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1021 10:10:47.812927   17860 system_pods.go:61] "metrics-server-84c5f94fbc-dxqmp" [2dc37bcd-0951-4b9d-a775-44b2c67c2b98] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1021 10:10:47.812927   17860 system_pods.go:61] "storage-provisioner" [edbeda38-f7e9-4579-b609-c7d287c69e63] Running
I1021 10:10:47.812927   17860 system_pods.go:74] duration metric: took 5.2742ms to wait for pod list to return data ...
I1021 10:10:47.812927   17860 kubeadm.go:582] duration metric: took 320.2892ms to wait for: map[apiserver:true system_pods:true]
I1021 10:10:47.812927   17860 node_conditions.go:102] verifying NodePressure condition ...
I1021 10:10:47.815106   17860 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I1021 10:10:47.815106   17860 node_conditions.go:123] node cpu capacity is 2
I1021 10:10:47.815106   17860 node_conditions.go:105] duration metric: took 2.179ms to run NodePressure ...
I1021 10:10:47.815106   17860 start.go:241] waiting for startup goroutines ...
I1021 10:10:48.136911   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.136911   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.136911   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.136911   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.137947   17860 out.go:177]   - Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I1021 10:10:48.137947   17860 out.go:177]   - Utilisation de l'image registry.k8s.io/metrics-server/metrics-server:v0.7.2
I1021 10:10:48.139009   17860 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1021 10:10:48.139009   17860 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1021 10:10:48.139009   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:48.139526   17860 addons.go:431] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I1021 10:10:48.139526   17860 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I1021 10:10:48.139526   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:48.156670   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.156670   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.157223   17860 out.go:177]   - Utilisation de l'image docker.io/kubernetesui/dashboard:v2.7.0
I1021 10:10:48.159089   17860 out.go:177]   - Utilisation de l'image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1021 10:10:48.160671   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1021 10:10:48.160671   17860 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1021 10:10:48.160842   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:48.173798   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.173798   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.175909   17860 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1021 10:10:48.175962   17860 addons.go:243] addon default-storageclass should already be in state true
I1021 10:10:48.175962   17860 host.go:66] Checking if "minikube" exists ...
I1021 10:10:48.176465   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:48.748263   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.748263   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.748263   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.748263   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.748263   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.748263   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.748263   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:48.748263   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:48.748263   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:48.761707   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:48.761707   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:48.761707   17860 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1021 10:10:48.761707   17860 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1021 10:10:48.761707   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1021 10:10:49.393826   17860 main.go:141] libmachine: [stdout =====>] : Running

I1021 10:10:49.393826   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:49.393826   17860 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1021 10:10:49.645959   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:49.645959   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:49.645959   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:49.658231   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:49.658231   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:49.658231   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:49.673061   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:49.673061   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:49.673061   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:49.731961   17860 addons.go:431] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I1021 10:10:49.731961   17860 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I1021 10:10:49.742852   17860 addons.go:431] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I1021 10:10:49.742852   17860 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I1021 10:10:49.754381   17860 addons.go:431] installing /etc/kubernetes/addons/metrics-server-service.yaml
I1021 10:10:49.754381   17860 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I1021 10:10:49.762824   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1021 10:10:49.762824   17860 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1021 10:10:49.767258   17860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1021 10:10:49.776910   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1021 10:10:49.776910   17860 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1021 10:10:49.786444   17860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I1021 10:10:49.789268   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1021 10:10:49.789268   17860 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1021 10:10:49.802598   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1021 10:10:49.802598   17860 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1021 10:10:49.817774   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-role.yaml
I1021 10:10:49.817774   17860 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1021 10:10:49.830421   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1021 10:10:49.830421   17860 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1021 10:10:49.841376   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1021 10:10:49.841376   17860 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1021 10:10:49.863294   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1021 10:10:49.863294   17860 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1021 10:10:49.875415   17860 addons.go:431] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1021 10:10:49.875415   17860 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1021 10:10:49.912119   17860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1021 10:10:50.125232   17860 main.go:141] libmachine: [stdout =====>] : 172.20.119.178

I1021 10:10:50.125232   17860 main.go:141] libmachine: [stderr =====>] : 
I1021 10:10:50.125232   17860 sshutil.go:53] new ssh client: &{IP:172.20.119.178 Port:22 SSHKeyPath:C:\Users\mecht\.minikube\machines\minikube\id_rsa Username:docker}
I1021 10:10:50.345342   17860 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1021 10:10:50.774104   17860 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0069877s)
I1021 10:10:50.774104   17860 addons.go:475] Verifying addon metrics-server=true in "minikube"
I1021 10:10:51.215793   17860 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.3038575s)
I1021 10:10:51.226439   17860 out.go:177] * Certaines fonctionnalités du tableau de bord nécessitent le module complémentaire metrics-server. Pour activer toutes les fonctionnalités, veuillez exécuter :

	minikube addons enable metrics-server

I1021 10:10:51.228281   17860 out.go:177] * Modules activés: storage-provisioner, metrics-server, default-storageclass, dashboard
I1021 10:10:51.236042   17860 addons.go:510] duration metric: took 3.7438863s for enable addons: enabled=[storage-provisioner metrics-server default-storageclass dashboard]
I1021 10:10:51.236042   17860 start.go:246] waiting for cluster config update ...
I1021 10:10:51.236231   17860 start.go:255] writing updated cluster config ...
I1021 10:10:51.247114   17860 ssh_runner.go:195] Run: rm -f paused
I1021 10:10:51.335555   17860 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1021 10:10:51.342122   17860 out.go:177] * Terminé ! kubectl est maintenant configuré pour utiliser "minikube" cluster et espace de noms "default" par défaut.


==> Docker <==
Oct 21 08:10:57 minikube dockerd[1037]: time="2024-10-21T08:10:57.994649109Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:57 minikube dockerd[1037]: time="2024-10-21T08:10:57.994712368Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:57 minikube dockerd[1037]: time="2024-10-21T08:10:57.994724166Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:57 minikube dockerd[1037]: time="2024-10-21T08:10:57.994784461Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.005187142Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.005351685Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.005392915Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.005490529Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube cri-dockerd[1304]: time="2024-10-21T08:10:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bacaf67aff98f7cf562dd93f47f710389942cf05aa19dbe6340938ab931693e6/resolv.conf as [nameserver 172.20.112.1]"
Oct 21 08:10:58 minikube cri-dockerd[1304]: time="2024-10-21T08:10:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d836b4cf1bdec50fc3b903e66ed07931552783fc0715d6634cfe631c90a853d/resolv.conf as [nameserver 172.20.112.1]"
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.186349940Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.186414020Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.186517497Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.186603029Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.189157320Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.189214681Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.189227083Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.189271187Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.397918762Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398219958Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398265793Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398277343Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398336345Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398411437Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398465671Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.398531127Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.405204659Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.405343913Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.405382013Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.405463009Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube cri-dockerd[1304]: time="2024-10-21T08:10:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a3deaac8c4853f43e3facb6cc2ed92179abfc99ad842c1755afc82e86ab8e7c/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 08:10:58 minikube cri-dockerd[1304]: time="2024-10-21T08:10:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a0d50f68208461cbda4a6e3614384d0371e80a62bda4d5f627da60869860b259/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 08:10:58 minikube cri-dockerd[1304]: time="2024-10-21T08:10:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a01aa08d59bb3c881d3380b8bf20910fb21b9c28128864c93962d0ac3f8dafb/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.651190099Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.651258390Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.651268697Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.651333042Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.749841198Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.749906358Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.749917972Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:10:58 minikube dockerd[1037]: time="2024-10-21T08:10:58.750112937Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:01 minikube cri-dockerd[1304]: time="2024-10-21T08:11:01Z" level=info msg="Stop pulling image blooomoo/k8s-microproject:latest: Status: Image is up to date for blooomoo/k8s-microproject:latest"
Oct 21 08:11:01 minikube dockerd[1037]: time="2024-10-21T08:11:01.572435458Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:11:01 minikube dockerd[1037]: time="2024-10-21T08:11:01.572685831Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:11:01 minikube dockerd[1037]: time="2024-10-21T08:11:01.572726293Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:01 minikube dockerd[1037]: time="2024-10-21T08:11:01.572819379Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:12 minikube dockerd[1031]: time="2024-10-21T08:11:12.875315431Z" level=info msg="ignoring event" container=e7bd4931e77dd67613e2ed433593f4402ee943c1970351f686fb84bf51cda6e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 08:11:12 minikube dockerd[1037]: time="2024-10-21T08:11:12.875412235Z" level=info msg="shim disconnected" id=e7bd4931e77dd67613e2ed433593f4402ee943c1970351f686fb84bf51cda6e6 namespace=moby
Oct 21 08:11:12 minikube dockerd[1037]: time="2024-10-21T08:11:12.875445210Z" level=warning msg="cleaning up after shim disconnected" id=e7bd4931e77dd67613e2ed433593f4402ee943c1970351f686fb84bf51cda6e6 namespace=moby
Oct 21 08:11:12 minikube dockerd[1037]: time="2024-10-21T08:11:12.875456167Z" level=info msg="cleaning up dead shim" namespace=moby
Oct 21 08:11:17 minikube dockerd[1037]: time="2024-10-21T08:11:17.236523664Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:11:17 minikube dockerd[1037]: time="2024-10-21T08:11:17.236591224Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:11:17 minikube dockerd[1037]: time="2024-10-21T08:11:17.236601581Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:17 minikube dockerd[1037]: time="2024-10-21T08:11:17.236682580Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:27 minikube dockerd[1037]: time="2024-10-21T08:11:27.249738814Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 21 08:11:27 minikube dockerd[1037]: time="2024-10-21T08:11:27.249797751Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 21 08:11:27 minikube dockerd[1037]: time="2024-10-21T08:11:27.249808809Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:27 minikube dockerd[1037]: time="2024-10-21T08:11:27.249888038Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 21 08:11:48 minikube cri-dockerd[1304]: time="2024-10-21T08:11:48Z" level=error msg="error getting RW layer size for container ID '5f510e7d2c8dc26cfe8f0b3a87a74805fec378a0c483db019d4aded8016a4895': Error response from daemon: No such container: 5f510e7d2c8dc26cfe8f0b3a87a74805fec378a0c483db019d4aded8016a4895"
Oct 21 08:11:48 minikube cri-dockerd[1304]: time="2024-10-21T08:11:48Z" level=error msg="Set backoffDuration to : 1m0s for container ID '5f510e7d2c8dc26cfe8f0b3a87a74805fec378a0c483db019d4aded8016a4895'"


==> container status <==
CONTAINER           IMAGE                                                                                               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
a72d2eb42e1d5       6e38f40d628db                                                                                       8 minutes ago       Running             storage-provisioner         8                   3e3ee571fde8d       storage-provisioner
fed14844e4e32       48d9cfaaf3904                                                                                       8 minutes ago       Running             metrics-server              3                   499a768312c67       metrics-server-84c5f94fbc-dxqmp
aff68da9c614a       blooomoo/k8s-microproject@sha256:ea5f4e6a5fc318caaa9ddeb883c3f92f5cb339a0e9d5e386a3071a11ef3bf18d   8 minutes ago       Running             k8s-miniprojet              4                   a0d50f6820846       k8s-miniprojet-deployment-76655588f8-spw4q
60b573b823f7c       07655ddf2eebe                                                                                       8 minutes ago       Running             kubernetes-dashboard        4                   5a01aa08d59bb       kubernetes-dashboard-695b96c756-jgcp6
9c367c5fadf03       115053965e86b                                                                                       8 minutes ago       Running             dashboard-metrics-scraper   4                   3a3deaac8c485       dashboard-metrics-scraper-c5db448b4-r8kwb
00716909958c3       cbb01a7bd410d                                                                                       8 minutes ago       Running             coredns                     4                   bacaf67aff98f       coredns-6f6b679f8f-x9v65
95b07474c43e1       cbb01a7bd410d                                                                                       8 minutes ago       Running             coredns                     4                   0d836b4cf1bde       coredns-6f6b679f8f-2fvhb
bf59222ac0885       48d9cfaaf3904                                                                                       9 minutes ago       Exited              metrics-server              2                   499a768312c67       metrics-server-84c5f94fbc-dxqmp
e7bd4931e77dd       6e38f40d628db                                                                                       9 minutes ago       Exited              storage-provisioner         7                   3e3ee571fde8d       storage-provisioner
8a31bfbd3bb04       ad83b2ca7b09e                                                                                       9 minutes ago       Running             kube-proxy                  4                   f1b53486a8475       kube-proxy-kgqrd
ff3009da2cd34       1766f54c897f0                                                                                       9 minutes ago       Running             kube-scheduler              5                   14587960e5aa3       kube-scheduler-minikube
c385036b9ad8b       045733566833c                                                                                       9 minutes ago       Running             kube-controller-manager     6                   6a44121f084ea       kube-controller-manager-minikube
e3eb5e3884d42       604f5db92eaa8                                                                                       9 minutes ago       Running             kube-apiserver              0                   8e283aec007ef       kube-apiserver-minikube
139f51d364dfe       2e96e5913fc06                                                                                       9 minutes ago       Running             etcd                        0                   c7b71ce74a51f       etcd-minikube
1f676b290a046       blooomoo/k8s-microproject@sha256:ea5f4e6a5fc318caaa9ddeb883c3f92f5cb339a0e9d5e386a3071a11ef3bf18d   30 minutes ago      Exited              k8s-miniprojet              3                   e3b08e8708e9e       k8s-miniprojet-deployment-76655588f8-spw4q
acf476fd82561       07655ddf2eebe                                                                                       30 minutes ago      Exited              kubernetes-dashboard        3                   d52ce901b5124       kubernetes-dashboard-695b96c756-jgcp6
0f354ac01583e       115053965e86b                                                                                       30 minutes ago      Exited              dashboard-metrics-scraper   3                   ce67de96b7ba3       dashboard-metrics-scraper-c5db448b4-r8kwb
38c04065d2b24       cbb01a7bd410d                                                                                       30 minutes ago      Exited              coredns                     3                   db876e4fa97e0       coredns-6f6b679f8f-2fvhb
3b47b27eca9ab       cbb01a7bd410d                                                                                       30 minutes ago      Exited              coredns                     3                   0fa830d21ea90       coredns-6f6b679f8f-x9v65
85eade8e9a80b       ad83b2ca7b09e                                                                                       30 minutes ago      Exited              kube-proxy                  3                   2bbd46733dfa3       kube-proxy-kgqrd
73abcdc61dfbf       045733566833c                                                                                       30 minutes ago      Exited              kube-controller-manager     5                   20f7a6faef2df       kube-controller-manager-minikube
e1eb455dd9de2       1766f54c897f0                                                                                       30 minutes ago      Exited              kube-scheduler              4                   c091ebae4d900       kube-scheduler-minikube


==> coredns [00716909958c] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> coredns [38c04065d2b2] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [3b47b27eca9a] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [95b07474c43e] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> describe nodes <==
Name:               minikube
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 21 Oct 2024 07:00:03 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 21 Oct 2024 08:19:52 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 21 Oct 2024 08:15:57 +0000   Mon, 21 Oct 2024 07:00:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 21 Oct 2024 08:15:57 +0000   Mon, 21 Oct 2024 07:00:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 21 Oct 2024 08:15:57 +0000   Mon, 21 Oct 2024 07:00:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 21 Oct 2024 08:15:57 +0000   Mon, 21 Oct 2024 08:10:50 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.20.119.178
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
System Info:
  Machine ID:                 7e3c343823d74d8d92ec0aa0c6a9c484
  System UUID:                a7b09904-fe3a-a345-98b8-5d7fd8b3d50b
  Boot ID:                    0826e53a-2ba8-42f3-9c15-6a232f39fe86
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     k8s-miniprojet-deployment-76655588f8-spw4q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m
  kube-system                 coredns-6f6b679f8f-2fvhb                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     79m
  kube-system                 coredns-6f6b679f8f-x9v65                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     79m
  kube-system                 etcd-minikube                                 100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         9m12s
  kube-system                 kube-apiserver-minikube                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         9m12s
  kube-system                 kube-controller-manager-minikube              200m (10%)    0 (0%)      0 (0%)           0 (0%)         79m
  kube-system                 kube-proxy-kgqrd                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m
  kube-system                 kube-scheduler-minikube                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         79m
  kube-system                 metrics-server-84c5f94fbc-dxqmp               100m (5%)     0 (0%)      200Mi (5%)       0 (0%)         38m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-r8kwb     0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-jgcp6         0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (47%)   0 (0%)
  memory             440Mi (11%)  340Mi (8%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 79m                    kube-proxy       
  Normal  Starting                 9m11s                  kube-proxy       
  Normal  Starting                 30m                    kube-proxy       
  Normal  Starting                 39m                    kube-proxy       
  Normal  Starting                 73m                    kube-proxy       
  Normal  NodeHasSufficientPID     79m (x7 over 79m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  79m (x8 over 79m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    79m (x8 over 79m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeAllocatableEnforced  79m                    kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 79m                    kubelet          Starting kubelet.
  Normal  RegisteredNode           79m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeHasSufficientMemory  73m (x8 over 73m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 73m                    kubelet          Starting kubelet.
  Normal  NodeHasNoDiskPressure    73m (x8 over 73m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     73m (x7 over 73m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  73m                    kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           73m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 40m                    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  40m                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     40m (x7 over 40m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    40m (x8 over 40m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  40m (x8 over 40m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           39m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 30m                    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  30m                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     30m (x7 over 30m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    30m (x8 over 30m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  30m (x8 over 30m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           30m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 9m16s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  9m16s (x8 over 9m16s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    9m16s (x8 over 9m16s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     9m16s (x7 over 9m16s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  9m16s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           9m6s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct21 08:10] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000000] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.050173] Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!
[  +0.034401] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.004622] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[  +3.300787] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.460795] psmouse serio1: trackpoint: failed to get extended button data, assuming 3 buttons
[  +0.725993] systemd-fstab-generator[115]: Ignoring "noauto" option for root device
[  +1.536698] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000005] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[ +13.541187] systemd-fstab-generator[620]: Ignoring "noauto" option for root device
[  +0.079463] systemd-fstab-generator[632]: Ignoring "noauto" option for root device
[  +6.732035] systemd-fstab-generator[959]: Ignoring "noauto" option for root device
[  +0.043767] kauditd_printk_skb: 71 callbacks suppressed
[  +0.364739] systemd-fstab-generator[997]: Ignoring "noauto" option for root device
[  +0.097222] systemd-fstab-generator[1009]: Ignoring "noauto" option for root device
[  +0.104487] systemd-fstab-generator[1023]: Ignoring "noauto" option for root device
[  +2.561118] systemd-fstab-generator[1257]: Ignoring "noauto" option for root device
[  +0.106008] systemd-fstab-generator[1269]: Ignoring "noauto" option for root device
[  +0.099005] systemd-fstab-generator[1281]: Ignoring "noauto" option for root device
[  +0.216049] systemd-fstab-generator[1296]: Ignoring "noauto" option for root device
[  +0.559868] systemd-fstab-generator[1423]: Ignoring "noauto" option for root device
[  +2.001677] systemd-fstab-generator[1542]: Ignoring "noauto" option for root device
[  +0.038673] kauditd_printk_skb: 224 callbacks suppressed
[  +5.015922] kauditd_printk_skb: 82 callbacks suppressed
[  +4.074840] systemd-fstab-generator[2248]: Ignoring "noauto" option for root device
[  +1.686273] kauditd_printk_skb: 38 callbacks suppressed
[  +7.084548] kauditd_printk_skb: 17 callbacks suppressed
[Oct21 08:11] kauditd_printk_skb: 79 callbacks suppressed


==> etcd [139f51d364df] <==
{"level":"warn","ts":"2024-10-21T08:10:39.022529Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-21T08:10:39.022908Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.20.119.178:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.20.119.178:2380","--initial-cluster=minikube=https://172.20.119.178:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.20.119.178:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.20.119.178:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-10-21T08:10:39.022966Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-10-21T08:10:39.022984Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-21T08:10:39.022991Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://172.20.119.178:2380"]}
{"level":"info","ts":"2024-10-21T08:10:39.023019Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-21T08:10:39.025510Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://172.20.119.178:2379"]}
{"level":"info","ts":"2024-10-21T08:10:39.026140Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://172.20.119.178:2380"],"listen-peer-urls":["https://172.20.119.178:2380"],"advertise-client-urls":["https://172.20.119.178:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.20.119.178:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-10-21T08:10:39.038781Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"12.178398ms"}
{"level":"info","ts":"2024-10-21T08:10:39.053535Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-10-21T08:10:39.068497Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"4c84978ff7565c85","local-member-id":"62c585ecd4cdd82e","commit-index":5297}
{"level":"info","ts":"2024-10-21T08:10:39.069445Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e switched to configuration voters=()"}
{"level":"info","ts":"2024-10-21T08:10:39.069493Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e became follower at term 5"}
{"level":"info","ts":"2024-10-21T08:10:39.069611Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 62c585ecd4cdd82e [peers: [], term: 5, commit: 5297, applied: 0, lastindex: 5297, lastterm: 5]"}
{"level":"warn","ts":"2024-10-21T08:10:39.072423Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-10-21T08:10:39.073414Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4142}
{"level":"info","ts":"2024-10-21T08:10:39.075149Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":4417}
{"level":"info","ts":"2024-10-21T08:10:39.076780Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-10-21T08:10:39.078544Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"62c585ecd4cdd82e","timeout":"7s"}
{"level":"info","ts":"2024-10-21T08:10:39.078953Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"62c585ecd4cdd82e"}
{"level":"info","ts":"2024-10-21T08:10:39.078995Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"62c585ecd4cdd82e","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-10-21T08:10:39.079176Z","caller":"etcdserver/server.go:767","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-10-21T08:10:39.079600Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-21T08:10:39.079685Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-21T08:10:39.079749Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-21T08:10:39.079962Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e switched to configuration voters=(7117242038357973038)"}
{"level":"info","ts":"2024-10-21T08:10:39.080002Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"4c84978ff7565c85","local-member-id":"62c585ecd4cdd82e","added-peer-id":"62c585ecd4cdd82e","added-peer-peer-urls":["https://172.20.115.87:2380"]}
{"level":"info","ts":"2024-10-21T08:10:39.080070Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"4c84978ff7565c85","local-member-id":"62c585ecd4cdd82e","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-21T08:10:39.080102Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-21T08:10:39.079684Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-21T08:10:39.082552Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-21T08:10:39.083625Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"62c585ecd4cdd82e","initial-advertise-peer-urls":["https://172.20.119.178:2380"],"listen-peer-urls":["https://172.20.119.178:2380"],"advertise-client-urls":["https://172.20.119.178:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.20.119.178:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-10-21T08:10:39.083773Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-10-21T08:10:39.083359Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"172.20.119.178:2380"}
{"level":"info","ts":"2024-10-21T08:10:39.084698Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"172.20.119.178:2380"}
{"level":"info","ts":"2024-10-21T08:10:40.670477Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e is starting a new election at term 5"}
{"level":"info","ts":"2024-10-21T08:10:40.670557Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e became pre-candidate at term 5"}
{"level":"info","ts":"2024-10-21T08:10:40.670620Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e received MsgPreVoteResp from 62c585ecd4cdd82e at term 5"}
{"level":"info","ts":"2024-10-21T08:10:40.670648Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e became candidate at term 6"}
{"level":"info","ts":"2024-10-21T08:10:40.670665Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e received MsgVoteResp from 62c585ecd4cdd82e at term 6"}
{"level":"info","ts":"2024-10-21T08:10:40.670688Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"62c585ecd4cdd82e became leader at term 6"}
{"level":"info","ts":"2024-10-21T08:10:40.670712Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 62c585ecd4cdd82e elected leader 62c585ecd4cdd82e at term 6"}
{"level":"info","ts":"2024-10-21T08:10:40.673456Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"62c585ecd4cdd82e","local-member-attributes":"{Name:minikube ClientURLs:[https://172.20.119.178:2379]}","request-path":"/0/members/62c585ecd4cdd82e/attributes","cluster-id":"4c84978ff7565c85","publish-timeout":"7s"}
{"level":"info","ts":"2024-10-21T08:10:40.673464Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-21T08:10:40.673592Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-10-21T08:10:40.673603Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-10-21T08:10:40.673556Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-21T08:10:40.674816Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-21T08:10:40.675317Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.20.119.178:2379"}
{"level":"info","ts":"2024-10-21T08:10:40.675798Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-21T08:10:40.676224Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 08:19:54 up 9 min,  0 users,  load average: 0.06, 0.06, 0.02
Linux minikube 5.10.207 #1 SMP Tue Sep 3 21:45:30 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [e3eb5e3884d4] <==
I1021 08:10:41.413562       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1021 08:10:41.415648       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1021 08:10:41.415648       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1021 08:10:41.420104       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1021 08:10:41.426297       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1021 08:10:41.426367       1 aggregator.go:171] initial CRD sync complete...
I1021 08:10:41.426381       1 autoregister_controller.go:144] Starting autoregister controller
I1021 08:10:41.426395       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1021 08:10:41.426407       1 cache.go:39] Caches are synced for autoregister controller
I1021 08:10:41.429515       1 shared_informer.go:320] Caches are synced for configmaps
I1021 08:10:41.454095       1 shared_informer.go:320] Caches are synced for node_authorizer
I1021 08:10:41.457426       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1021 08:10:41.457569       1 policy_source.go:224] refreshing policies
I1021 08:10:41.479194       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1021 08:10:42.313529       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1021 08:10:46.420913       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E1021 08:10:46.429405       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
W1021 08:10:46.519666       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.20.118.161 172.20.119.178]
I1021 08:10:46.520542       1 controller.go:615] quota admission added evaluator for: endpoints
I1021 08:10:46.523587       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1021 08:10:46.791104       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1021 08:10:46.798433       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1021 08:10:46.827586       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1021 08:10:46.965189       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1021 08:10:46.970343       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E1021 08:10:49.525337       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I1021 08:10:49.526414       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1021 08:10:51.446876       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.107.126.199:443: i/o timeout" logger="UnhandledError"
E1021 08:10:56.565298       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.107.126.199:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1021 08:10:56.565746       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.107.126.199:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1021 08:10:56.566804       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1021 08:10:56.566897       1 handler_proxy.go:99] no RequestInfo found in the context
E1021 08:10:56.566928       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W1021 08:10:57.567778       1 handler_proxy.go:99] no RequestInfo found in the context
E1021 08:10:57.567827       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I1021 08:10:57.568937       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1021 08:10:57.568983       1 handler_proxy.go:99] no RequestInfo found in the context
E1021 08:10:57.569046       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1021 08:10:57.570126       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1021 08:11:06.530884       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.20.119.178]
W1021 08:11:17.998646       1 handler_proxy.go:99] no RequestInfo found in the context
E1021 08:11:17.999246       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1021 08:11:17.999200       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.107.126.199:443: connect: connection refused" logger="UnhandledError"
E1021 08:11:18.001051       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.107.126.199:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.107.126.199:443: connect: connection refused" logger="UnhandledError"
I1021 08:11:18.029888       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager


==> kube-controller-manager [73abcdc61dfb] <==
I1021 07:49:08.092609       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1021 07:49:08.092935       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1021 07:49:08.098234       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="121.357616ms"
I1021 07:49:08.101792       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="181.817µs"
I1021 07:49:08.109004       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="104.22862ms"
I1021 07:49:08.109374       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="158.699µs"
I1021 07:49:08.110849       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="101.835467ms"
I1021 07:49:08.111201       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="225.03µs"
I1021 07:49:08.114236       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="120.492531ms"
I1021 07:49:08.114906       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="98.59µs"
I1021 07:49:08.116751       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="108.162946ms"
I1021 07:49:08.120356       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="93.682µs"
I1021 07:49:08.121731       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1021 07:49:08.125005       1 shared_informer.go:320] Caches are synced for GC
I1021 07:49:08.138421       1 shared_informer.go:320] Caches are synced for node
I1021 07:49:08.139120       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1021 07:49:08.139269       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1021 07:49:08.139290       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1021 07:49:08.139315       1 shared_informer.go:320] Caches are synced for cidrallocator
I1021 07:49:08.139574       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 07:49:08.145236       1 shared_informer.go:320] Caches are synced for taint
I1021 07:49:08.145628       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1021 07:49:08.145831       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1021 07:49:08.145980       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1021 07:49:08.159798       1 shared_informer.go:320] Caches are synced for attach detach
I1021 07:49:08.161931       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1021 07:49:08.162028       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 07:49:08.166189       1 shared_informer.go:320] Caches are synced for daemon sets
I1021 07:49:08.174164       1 shared_informer.go:320] Caches are synced for persistent volume
I1021 07:49:08.177967       1 shared_informer.go:320] Caches are synced for TTL
I1021 07:49:08.185305       1 shared_informer.go:320] Caches are synced for resource quota
I1021 07:49:08.202657       1 shared_informer.go:320] Caches are synced for endpoint
I1021 07:49:08.206256       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1021 07:49:08.226871       1 shared_informer.go:320] Caches are synced for resource quota
I1021 07:49:08.665944       1 shared_informer.go:320] Caches are synced for garbage collector
I1021 07:49:08.672287       1 shared_informer.go:320] Caches are synced for garbage collector
I1021 07:49:08.672346       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1021 07:49:08.989295       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 07:49:08.990022       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube"
I1021 07:49:09.003809       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 07:49:10.654554       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="34.935µs"
I1021 07:49:12.686519       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="11.764706ms"
I1021 07:49:12.687400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="70.381µs"
I1021 07:49:13.147280       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1021 07:49:17.772784       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="47.793µs"
I1021 07:49:17.838786       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="25.253839ms"
I1021 07:49:17.841630       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="77.778µs"
I1021 07:49:17.860525       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="7.194134ms"
I1021 07:49:17.860581       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="29.667µs"
I1021 07:49:17.890609       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="12.896396ms"
I1021 07:49:17.890670       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="38.377µs"
I1021 07:49:18.838804       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="7.298978ms"
I1021 07:49:18.839822       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="42.228µs"
I1021 07:49:21.899675       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="21.371352ms"
I1021 07:49:21.899771       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="33.249µs"
I1021 07:54:16.780718       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 07:59:23.555630       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:04:28.694826       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:08:02.195623       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:08:32.793348       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [c385036b9ad8] <==
I1021 08:10:48.675253       1 shared_informer.go:320] Caches are synced for persistent volume
I1021 08:10:48.679958       1 shared_informer.go:320] Caches are synced for deployment
I1021 08:10:48.683335       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1021 08:10:48.687570       1 shared_informer.go:320] Caches are synced for daemon sets
I1021 08:10:48.687663       1 shared_informer.go:320] Caches are synced for job
I1021 08:10:48.691434       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:10:48.696476       1 shared_informer.go:320] Caches are synced for HPA
I1021 08:10:48.699135       1 shared_informer.go:320] Caches are synced for stateful set
I1021 08:10:48.700890       1 shared_informer.go:320] Caches are synced for crt configmap
I1021 08:10:48.700897       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1021 08:10:48.700906       1 shared_informer.go:320] Caches are synced for TTL after finished
I1021 08:10:48.706221       1 shared_informer.go:320] Caches are synced for namespace
I1021 08:10:48.714064       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1021 08:10:48.714125       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1021 08:10:48.714257       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1021 08:10:48.714273       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1021 08:10:48.754220       1 shared_informer.go:320] Caches are synced for cronjob
I1021 08:10:48.777072       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1021 08:10:48.800855       1 shared_informer.go:320] Caches are synced for endpoint
I1021 08:10:48.800924       1 shared_informer.go:320] Caches are synced for attach detach
I1021 08:10:48.821635       1 shared_informer.go:320] Caches are synced for resource quota
I1021 08:10:48.836734       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:10:48.900738       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1021 08:10:48.912140       1 shared_informer.go:320] Caches are synced for resource quota
I1021 08:10:48.968393       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="330.58805ms"
I1021 08:10:48.969066       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="21.429µs"
I1021 08:10:48.970963       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="331.682547ms"
I1021 08:10:48.972120       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="1.096618ms"
I1021 08:10:48.972326       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="333.965464ms"
I1021 08:10:48.972429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="54.526µs"
I1021 08:10:48.972692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="332.880206ms"
I1021 08:10:48.972879       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="31.678µs"
I1021 08:10:48.972968       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="333.269534ms"
I1021 08:10:48.973055       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="26.383µs"
I1021 08:10:49.327893       1 shared_informer.go:320] Caches are synced for garbage collector
I1021 08:10:49.327929       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1021 08:10:49.333259       1 shared_informer.go:320] Caches are synced for garbage collector
I1021 08:10:50.544579       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube"
I1021 08:10:50.544630       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:10:50.553930       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:10:52.603099       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="31.022µs"
I1021 08:10:53.844894       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1021 08:10:53.845604       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1021 08:10:56.636921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="42.173µs"
I1021 08:10:58.676485       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="1.599247ms"
I1021 08:10:58.716746       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="15.280515ms"
I1021 08:10:58.718000       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="86.554µs"
I1021 08:10:58.736371       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="10.254374ms"
I1021 08:10:58.736633       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="43.047µs"
I1021 08:10:58.816541       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="6.467286ms"
I1021 08:10:58.817120       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="32.36µs"
I1021 08:10:59.839339       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="7.839899ms"
I1021 08:10:59.839451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="39.149µs"
I1021 08:11:01.860954       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="6.895612ms"
I1021 08:11:01.861941       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8s-miniprojet-deployment-76655588f8" duration="17.122µs"
I1021 08:11:02.207314       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="78.669µs"
I1021 08:11:17.978959       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="37.768µs"
I1021 08:11:17.994255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="7.182804ms"
I1021 08:11:17.994299       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-84c5f94fbc" duration="24.345µs"
I1021 08:15:57.468863       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [85eade8e9a80] <==
I1021 07:49:01.775904       1 server_linux.go:66] "Using iptables proxy"
E1021 07:49:01.833537       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E1021 07:49:01.855154       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I1021 07:49:01.898620       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.20.118.161"]
E1021 07:49:01.898803       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1021 07:49:01.927865       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1021 07:49:01.927946       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1021 07:49:01.928208       1 server_linux.go:169] "Using iptables Proxier"
I1021 07:49:01.930017       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1021 07:49:01.930885       1 server.go:483] "Version info" version="v1.31.0"
I1021 07:49:01.930972       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1021 07:49:01.934089       1 config.go:197] "Starting service config controller"
I1021 07:49:01.934263       1 shared_informer.go:313] Waiting for caches to sync for service config
I1021 07:49:01.934134       1 config.go:104] "Starting endpoint slice config controller"
I1021 07:49:01.934620       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1021 07:49:01.935043       1 config.go:326] "Starting node config controller"
I1021 07:49:01.935112       1 shared_informer.go:313] Waiting for caches to sync for node config
I1021 07:49:02.035349       1 shared_informer.go:320] Caches are synced for node config
I1021 07:49:02.035627       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1021 07:49:02.035630       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [8a31bfbd3bb0] <==
I1021 08:10:42.938506       1 server_linux.go:66] "Using iptables proxy"
E1021 08:10:42.963581       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E1021 08:10:42.970803       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I1021 08:10:42.983864       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.20.119.178"]
E1021 08:10:42.984072       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1021 08:10:43.004088       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1021 08:10:43.004132       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1021 08:10:43.004154       1 server_linux.go:169] "Using iptables Proxier"
I1021 08:10:43.005674       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1021 08:10:43.005944       1 server.go:483] "Version info" version="v1.31.0"
I1021 08:10:43.005964       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1021 08:10:43.008624       1 config.go:197] "Starting service config controller"
I1021 08:10:43.008931       1 config.go:104] "Starting endpoint slice config controller"
I1021 08:10:43.009378       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1021 08:10:43.009380       1 shared_informer.go:313] Waiting for caches to sync for service config
I1021 08:10:43.010988       1 config.go:326] "Starting node config controller"
I1021 08:10:43.010996       1 shared_informer.go:313] Waiting for caches to sync for node config
I1021 08:10:43.110036       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1021 08:10:43.110048       1 shared_informer.go:320] Caches are synced for service config
I1021 08:10:43.111124       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [e1eb455dd9de] <==
I1021 07:48:59.167038       1 serving.go:386] Generated self-signed cert in-memory
W1021 07:49:00.592880       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1021 07:49:00.593055       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1021 07:49:00.593112       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1021 07:49:00.593141       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1021 07:49:00.624593       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1021 07:49:00.627714       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1021 07:49:00.630500       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1021 07:49:00.632058       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1021 07:49:00.631901       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1021 07:49:00.632617       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1021 07:49:00.732759       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1021 08:09:42.882599       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I1021 08:09:42.882632       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E1021 08:09:42.882756       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [ff3009da2cd3] <==
I1021 08:10:40.296297       1 serving.go:386] Generated self-signed cert in-memory
W1021 08:10:41.344417       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1021 08:10:41.344441       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1021 08:10:41.344449       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1021 08:10:41.344455       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1021 08:10:41.390653       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1021 08:10:41.390679       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1021 08:10:41.392234       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1021 08:10:41.392247       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1021 08:10:41.392279       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1021 08:10:41.393156       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1021 08:10:41.499542       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Oct 21 08:10:50 minikube kubelet[1549]: E1021 08:10:50.193224    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/coredns-6f6b679f8f-x9v65" podUID="0c3f202c-c074-464c-b69b-50411667ba4f"
Oct 21 08:10:50 minikube kubelet[1549]: E1021 08:10:50.193249    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized" pod="kube-system/metrics-server-84c5f94fbc-dxqmp" podUID="2dc37bcd-0951-4b9d-a775-44b2c67c2b98"
Oct 21 08:10:50 minikube kubelet[1549]: I1021 08:10:50.529594    1549 kubelet_node_status.go:488] "Fast updating node status as it just became ready"
Oct 21 08:10:56 minikube kubelet[1549]: I1021 08:10:56.626296    1549 scope.go:117] "RemoveContainer" containerID="5f92148b60de2ab8ee4ad39c95db6e285456c3e16eec319f635a8562a68eca67"
Oct 21 08:10:56 minikube kubelet[1549]: I1021 08:10:56.626493    1549 scope.go:117] "RemoveContainer" containerID="bf59222ac0885f204c16e4fcb3a80a2402e1b3562b5f97ae5a5bd3cae2b727fe"
Oct 21 08:10:56 minikube kubelet[1549]: E1021 08:10:56.626588    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 10s restarting failed container=metrics-server pod=metrics-server-84c5f94fbc-dxqmp_kube-system(2dc37bcd-0951-4b9d-a775-44b2c67c2b98)\"" pod="kube-system/metrics-server-84c5f94fbc-dxqmp" podUID="2dc37bcd-0951-4b9d-a775-44b2c67c2b98"
Oct 21 08:11:02 minikube kubelet[1549]: I1021 08:11:02.196734    1549 scope.go:117] "RemoveContainer" containerID="bf59222ac0885f204c16e4fcb3a80a2402e1b3562b5f97ae5a5bd3cae2b727fe"
Oct 21 08:11:02 minikube kubelet[1549]: E1021 08:11:02.196846    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 10s restarting failed container=metrics-server pod=metrics-server-84c5f94fbc-dxqmp_kube-system(2dc37bcd-0951-4b9d-a775-44b2c67c2b98)\"" pod="kube-system/metrics-server-84c5f94fbc-dxqmp" podUID="2dc37bcd-0951-4b9d-a775-44b2c67c2b98"
Oct 21 08:11:12 minikube kubelet[1549]: I1021 08:11:12.920999    1549 scope.go:117] "RemoveContainer" containerID="9cfa5fc12a067d90cb707f98c1b1574db04d66db56a5d70d46ad30d8ab0f041a"
Oct 21 08:11:12 minikube kubelet[1549]: I1021 08:11:12.921223    1549 scope.go:117] "RemoveContainer" containerID="e7bd4931e77dd67613e2ed433593f4402ee943c1970351f686fb84bf51cda6e6"
Oct 21 08:11:12 minikube kubelet[1549]: E1021 08:11:12.921305    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(edbeda38-f7e9-4579-b609-c7d287c69e63)\"" pod="kube-system/storage-provisioner" podUID="edbeda38-f7e9-4579-b609-c7d287c69e63"
Oct 21 08:11:17 minikube kubelet[1549]: I1021 08:11:17.192346    1549 scope.go:117] "RemoveContainer" containerID="bf59222ac0885f204c16e4fcb3a80a2402e1b3562b5f97ae5a5bd3cae2b727fe"
Oct 21 08:11:27 minikube kubelet[1549]: I1021 08:11:27.193511    1549 scope.go:117] "RemoveContainer" containerID="e7bd4931e77dd67613e2ed433593f4402ee943c1970351f686fb84bf51cda6e6"
Oct 21 08:11:38 minikube kubelet[1549]: E1021 08:11:38.202664    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:11:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:11:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:11:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:11:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:11:38 minikube kubelet[1549]: I1021 08:11:38.399978    1549 scope.go:117] "RemoveContainer" containerID="dde92f1b3a2c0cd6a293b7f183d131b53f084749e1bb4e6917ee0507db9841aa"
Oct 21 08:11:38 minikube kubelet[1549]: I1021 08:11:38.432840    1549 scope.go:117] "RemoveContainer" containerID="5f510e7d2c8dc26cfe8f0b3a87a74805fec378a0c483db019d4aded8016a4895"
Oct 21 08:12:38 minikube kubelet[1549]: E1021 08:12:38.205790    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:12:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:12:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:12:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:12:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:13:38 minikube kubelet[1549]: E1021 08:13:38.204200    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:13:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:13:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:13:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:13:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:14:38 minikube kubelet[1549]: E1021 08:14:38.209474    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:14:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:14:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:14:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:14:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:15:38 minikube kubelet[1549]: E1021 08:15:38.207054    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:15:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:15:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:15:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:15:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:16:38 minikube kubelet[1549]: E1021 08:16:38.204549    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:16:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:16:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:16:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:16:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:17:38 minikube kubelet[1549]: E1021 08:17:38.203713    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:17:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:17:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:17:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:17:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:18:38 minikube kubelet[1549]: E1021 08:18:38.203468    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:18:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:18:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:18:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:18:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 21 08:19:38 minikube kubelet[1549]: E1021 08:19:38.203668    1549 iptables.go:577] "Could not set up iptables canary" err=<
Oct 21 08:19:38 minikube kubelet[1549]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 21 08:19:38 minikube kubelet[1549]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 21 08:19:38 minikube kubelet[1549]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 21 08:19:38 minikube kubelet[1549]:  > table="nat" chain="KUBE-KUBELET-CANARY"


==> kubernetes-dashboard [60b573b823f7] <==
2024/10/21 08:10:58 Starting overwatch
2024/10/21 08:10:58 Using namespace: kubernetes-dashboard
2024/10/21 08:10:58 Using in-cluster config to connect to apiserver
2024/10/21 08:10:58 Using secret token for csrf signing
2024/10/21 08:10:58 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/10/21 08:10:58 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/10/21 08:10:58 Successful initial request to the apiserver, version: v1.31.0
2024/10/21 08:10:58 Generating JWE encryption key
2024/10/21 08:10:58 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/10/21 08:10:58 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/10/21 08:10:58 Initializing JWE encryption key from synchronized object
2024/10/21 08:10:58 Creating in-cluster Sidecar client
2024/10/21 08:10:58 Serving insecurely on HTTP port: 9090
2024/10/21 08:10:58 Successful request to sidecar


==> kubernetes-dashboard [acf476fd8256] <==
2024/10/21 07:49:17 Starting overwatch
2024/10/21 07:49:17 Using namespace: kubernetes-dashboard
2024/10/21 07:49:17 Using in-cluster config to connect to apiserver
2024/10/21 07:49:17 Using secret token for csrf signing
2024/10/21 07:49:17 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/10/21 07:49:17 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/10/21 07:49:18 Successful initial request to the apiserver, version: v1.31.0
2024/10/21 07:49:18 Generating JWE encryption key
2024/10/21 07:49:18 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/10/21 07:49:18 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/10/21 07:49:18 Initializing JWE encryption key from synchronized object
2024/10/21 07:49:18 Creating in-cluster Sidecar client
2024/10/21 07:49:18 Successful request to sidecar
2024/10/21 07:49:18 Serving insecurely on HTTP port: 9090


==> storage-provisioner [a72d2eb42e1d] <==
I1021 08:11:27.292091       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1021 08:11:27.301558       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1021 08:11:27.302070       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1021 08:11:44.708934       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1021 08:11:44.709485       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_84f045d7-1b9e-464a-9b11-405a1cc5d2e4!
I1021 08:11:44.709634       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4f7a987c-a6ae-407f-a574-fa4647ca8b29", APIVersion:"v1", ResourceVersion:"4710", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_84f045d7-1b9e-464a-9b11-405a1cc5d2e4 became leader
I1021 08:11:44.811245       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_84f045d7-1b9e-464a-9b11-405a1cc5d2e4!


==> storage-provisioner [e7bd4931e77d] <==
I1021 08:10:42.863129       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1021 08:11:12.868529       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

